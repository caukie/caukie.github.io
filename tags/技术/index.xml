<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>技术 on cake918小屋</title>
    <link>https://www.yangwenchao.com/tags/%E6%8A%80%E6%9C%AF/</link>
    <description>Recent content in 技术 on cake918小屋</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 24 Jan 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://www.yangwenchao.com/tags/%E6%8A%80%E6%9C%AF/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>严选API网关构建实践</title>
      <link>https://www.yangwenchao.com/post/tech/2020/2018-12-25-wangguan1/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.yangwenchao.com/post/tech/2020/2018-12-25-wangguan1/</guid>
      <description>此处应有背景 近几年微服务架构如日中天，微服务化亦是大势所趋，严选的微服务化改造也已初具规模，基于Sidecar模式的服务网格渐趋稳定，其基本架构如下：（图片引自网络） 数据面部分，Sidecar代理采用nginx，并基于nginx扩展，包括服务发现，负载均衡，健康检查等。其控制面为Consul管理平台。 同时部分sidecar功能，也被嵌入到tomcat容器或第三方Client中，如链路跟踪，流量采样等，其控制面为apolloY配置中心。 关于Sidecar的介绍，没有比原图更直观的了！（图片引自网络） 与此同时，作为微服务架构中不可或缺的组成部分：API Gateway，也就应运而生。
合适的才是最好的 根据活跃度、语言、支持功能、上手难易等，对开源社区（Tky，MicroGateway，Zuul，Kong等）进行比较分析。 除了网关基本功能之外，我们更关注于性能以及可扩展性，因此倾向于非Java的实现，同时考虑到入门门槛、后续的发展，最终锁定了Kong。 Kong的详细介绍，参考官方文档：https://konghq.com/kong-community-edition/。 Openresty部分，从原理到应用，可参看官方文档。
一切从源码开始 从官方Fork了当时最新版本0.2.3（截至本文书写时，官方已更新至1.0.0版本），参考编译安装手册，依次安装Openresty，Luarocks，Kong，PostgreSQL。除了LUA环境、Openssl依赖小有波澜，一切进展顺利，首个进程轻松启动。 良好的开始是成功的一半！实践证明，这里只是整个进度条的1%！之后我们优化了Openresty的依赖模块，标准化了环境变量的路径以及应用的安装路径，最终输出了严选API网关的编译安装手册以及自动化脚本。
大道至简，天下归一 在已有场景下，一个应用节点更新，首先需要到Consul平台进行下线（很nice），而另外还需要SA协助从入口Nginx调整域名的Upstream配置（很简陋）。一者，服务更新的复杂度高，再者，额外依赖了SA的操作。结果就是导致服务更新的笨重！因此，网关引入consul-module，集成Sidecar的服务注册发现功能，之后，不论访问方式如何，业务只需要关心Consul平台的配置即可，简单易行，皆大欢喜。 同时，已有Kong配置，缺少对API的有效标识，在严选Consul架构下，网关对单个Location进行有效的定义：产品名+服务名+接口名，一方面，将无意义的Location进行命名，方便记忆；另一方面，根据定义好的名字，可以快速检索配置。
物以类聚，分而治之 集群部署隔离 商业版提供了完整的管理平台，对集群、插件、监控等进行管理。而严选环境下，我们也需要根据网络环境、使用场景、入口业务进行集群区分，进而实现资源隔离！ 根据网络环境，目前区分了：测试网关集群，内网网关集群，外网网关集群等； 根据使用场景，目前区分了：出口网关集群，ABTest网关集群，压测网关集群等； 根据入口业务，目前区分了：主站网关集群，活动网关集群，Openapi网关集群等； 如上图所示，实际我们先根据网络环境区分大集群，然后根据业务、场景再细分子集群。
接口访问隔离  业务接口的访问：  依赖于身份验证插件，进行接口级别的租户信息验证。为此，我们扩充了租户管理，可以配置租户的验证策略，以及租户级别的频控等策略。
 管理接口的暴露：  仅支持本地IP访问，且只暴露监控数据的获取接口；
配置管理隔离 基于严选已有的权限中心，我们结合API网关的配置页面，细化了面向API网关的访问权限。
 用户：  可以走openpid进行登录的所有corp账号，创建是需要指定所属的角色；
 角色：  可以定制不同角色可见的模块，比如业务方只需要查看监控模块、插件配置，不可进行操作；
 群组：  API创建时，需要指定其所属群组，群组下可以添加具体的用户，此时，该用户只能看到该群组下的API配置信息； 如上图所示， 用户 &amp;ndash; 角色 &amp;ndash; API：操作权限控制 用户 &amp;ndash; 群组 &amp;ndash; API：API业务隔离
战战兢兢，如履薄冰 可维护 作为所有业务的总入口，一旦有风吹草动，那必然是一石激起千层浪！除了服务稳定性，我们把日常的运维工作作为重中之重！
 统计报表  多维度实时查看线上流量、响应错误码、响应时间、TCP链接数等，方便第一时间确认问题点。
 监控告警  基于实时平台，对异常日志、错误请求进行实时监控。 基于运维平台，对系统基础资源，包括CPU，内存，磁盘等进行实时监控。 一旦出现异常及时告警，目前我们依赖了锦衣卫以及邮件进行相关的告警通知。 由于网关数据流量大，在此仅对异常请求数据进行入库，目前使用Elasticsearch进行存储，方便问题的定位分析。</description>
    </item>
    
    <item>
      <title>严选风控构建之路 --雄关漫道真如铁</title>
      <link>https://www.yangwenchao.com/post/tech/2020/2018-01-24-fengkong1/</link>
      <pubDate>Wed, 24 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>https://www.yangwenchao.com/post/tech/2020/2018-01-24-fengkong1/</guid>
      <description>本文旨在对严选风控平台搭建的具体实现历程做更详细的说明。
回想五月初，还在跟财务的妹子们聊的不亦乐乎，突然就被拉回到了一堆糙汉子中间，世界暗淡无光。也就在此时，我听到了“风控”，之后便一直与之纠缠不清，不能释怀，书记递给了我一份python代码，捧过这卷风控奥义，我仿佛看见了上帝。其实，还有一个图，是这样的 WTF，这是什么玩意儿！看到书记狡黠的微笑，我明白了，这个坑有点大，这个锅有点重，这条路很漫长！
 第一个念想，看看外面有没有的借鉴（抄袭）！京东、美团、爱奇艺、携程&amp;hellip;&amp;hellip; 都感觉能抄，但其实什么也抄不到，业界的说法基本类似，画的图也基本雷同，我艰难的抉择并复制了几个图，一个月后，我看到的仍然就是那几个图，他们静静的躺在我的doc里面。感觉风控很简单，这几张图就包含了所有，感觉风控很复杂，我仍然不知该从何处下手！恩，没错，他们的图，大概是下面这样子的， 此时，我再次拾起了书记的那份python代码，虽然看上去很挫，但是很实用，反复把玩，总归有个可以下手的地方。一番努力（其实一天足够了），python变成了java，感觉趁手了许多。但是，它仍旧只能躺在代码仓库里。几天后，炬少走过来跟我说，我这里有个数据平台叫Flink，很好用，实时计算的，你可以试试哦！我也就信了，后来伴随着数据平台的N次迁移改造，我也明白过来，我不就是那只小白鼠么！当然，感谢炬少，风控的demo版，好歹在数据平台的demo版上跑起来了。风控的晨曦已经足够迷人，我们的0.1版也正式扬帆起航。对，就是下面这样子的， 之后，我又重新翻起了doc那几张图，“规则引擎”，“计数中心”，“配置中心”&amp;hellip;&amp;hellip;一个一个模块渐渐浮出水面，清晰异常，只需按图索骥，各个击破。规则引擎的选择，仅仅根据官方说明，以及各个论坛上只言片语的介绍，只能是一脸茫然，干脆选取最方便上手的drools进行实施，目前看来运行稳定，性能也足够。虽然事后有看到说其性能不行等等论调，但实践证明，只要适合自己的业务场景需求，那就是最好的。当然，在经历双十一之后，暴露出drools的灵活性不尽如人意，进而我们又引入了另一个业界常用的groovy作为补充，双剑合璧，也就形成了现在的规则系统，取drools的模板化做频率规则，取groovy的动态化做定制策略，两者相辅相成。
 八月伊始，新鲜的血液流入，风控项目也算是正式建立起来，各个组件分拆实现，风控工作台开发等，开始如火如荼搞起来了。配置中心、计数中心、信誉中心、数据检索、报表展示、数据访问层等等也都在这个阶段明确并固化，各个业务也开始有条不紊的接入。在新系统中，新的规则应用，初期的效果非常振奋人心，此时风控才真正崭露头角，小试牛刀。此时整个系统基本成型，是这个样子的， 九月初，频率的识别量日渐式微，基于模型的识别迫在眉睫，在炬少的大力支持推动下，机器学习也开始正式提上日程，也就在此时识别中心和拦截中心这两个概念才明确区分，基于模型的识别又分为离线模型、准实时模型、实时模型，具体模型的构建也稳步前行，并在实际拦截的占比中逐步提升，目前已经超出了90%，成为识别的主力军，后续主要工作也将集中于更多的数据分析、模型的构建、优化上面。
 十月后，系统主要功能已趋于完善，我们重点考虑服务的可靠性，分拆出了风控接入层，应对不同业务接入时，避免对核心业务造成影响，同时也将业务在接入层进行隔离，避免相互影响。同时也搭建了风控系统自身的压测环境，并得出了相关的性能基线，作为后续接入业务的基准。此时我们也开始考虑风控系统自身的稳定性，可靠性，容灾，性能调优等等。比如，对整个系统的物理部署拓扑做了规划，从物理上进行了隔离，并引入了流量控制、灰度控制等逻辑。针对所有接入业务的性能峰值进行评估，并对所有可能出问题的点做了预案，万事俱备，只欠一场真正的考验！
 双十一期间，风控严阵以待，实践证明之前的预案设计是行之有效的，各个业务的流量曲线尽在掌握之中。但是，为应对临时的线上问题，突发的增加了一些规则，也给系统引入了不可控因素。因此，双十一之后，重点对规则的添加、修改流程做了完善，规范了规则上线步骤，增加验证和线上回归等阶段，确保线上的调整可控。同时，还补充了规则的命中率监控，小单监控等，确保可以实时感知到线上的波动，及时作出响应。
 道高一尺魔高一丈，这是一场无止境的战争，风控系统一直在提升刷子的难度，但是无法杜绝刷子。而刷子也会绞尽脑汁，或脑洞大开，总有办法绕过我们的防御，一旦我们响应不够及时，他们就会趁机涌入千军万马。为此，我们又在这之后构筑了一道新的防线：事后仲裁！这里支持自动识别和人工判别，对绕过的危险分子进行再次识别，对公司的资产的流失进行二次防护。当然，除风控系统自身，我们还可以根据客服，物流等的反馈，再次兜底。恩，最终版本，是下面这样子的，  当然，风控的大厦并未就此完结。一者，随着越来越多业务的接入，也需要风控系统能够灵活的应对各个业务的变化；再者，刷子的无限想象力是我们后续进步的动力，需要能够快速响应，包括新特征的分析，新模型的构建等。雄关漫道真如铁，而今迈步从头越，简单的回忆了下风控构建历程，与诸位共勉！</description>
    </item>
    
  </channel>
</rss>